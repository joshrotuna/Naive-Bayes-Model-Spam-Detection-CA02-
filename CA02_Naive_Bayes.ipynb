{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA02_Joshua_Rotuna.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZYXwtCsL_y"
      },
      "source": [
        "This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4p_DvtT7sOIr",
        "outputId": "c5475ae7-85e9-454d-8993-33cc87142041"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Mount Google Drive (Because Colab is working from the cloud)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD79yQrGhMu0"
      },
      "source": [
        "## Setting Directories\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nljTElLHATD"
      },
      "source": [
        "TRAIN_directory = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/train-mails'\n",
        "TEST_directory = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/test-mails'"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZb03ilaGcb7"
      },
      "source": [
        "## Cleaning & Preparing the Data\n",
        "\n",
        "Finding 3000 most common words\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXknSIrLvzfQ"
      },
      "source": [
        "This function builds a Dictionary of most common 3000 words from all the \n",
        "email content. It eliminates \"stop words\" such as is, this, are, that doen contribute much to the anlaysis.\n",
        "\n",
        "First it adds all words and symbols in the dictionary. Then it removes all non-alpha-numeric characters and any single character alpha-numeric characters. After this is complete it shrinks the Dictionary by keeping only most common 3000 words in the dictionary. It returns the Dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jjKF0nIMwz8_"
      },
      "source": [
        "def my_dictionary(main_directory):\n",
        "    # Create dictionary called make_Dictionary that looks for 'root_dir'\n",
        "  word_list = []\n",
        "    # Makes a blank list to store the words in later\n",
        "  email_list = [os.path.join(main_directory,f) for f in os.listdir(main_directory)]\n",
        "    # Joins path components. Concatenates various path compnents with one directory separator\n",
        "    # os.path.join(path, *paths) ; joins one or more path components/directories \n",
        "    # f = number of files\n",
        "    # emails is a list of all of the files\n",
        "  for message in email_list:\n",
        "    # Iterates through every element in emails\n",
        "    with open(message) as m:\n",
        "      # Opens each email, defined as 'm'\n",
        "      # Looks through each email independently and closes it after\n",
        "      for each_line in m:\n",
        "      # looks through each line in each email from \"emails\"\n",
        "        email_text = each_line.split()\n",
        "      # splits the email at the spaces to extract each word\n",
        "        word_list += email_text\n",
        "      # Adds a value and a variable and assigns the results to that variable\n",
        "  dictionary = Counter(word_list)\n",
        "      # is a container that keeps track of how many times equivalent values are added. (Similar to value_counts or unique function)\n",
        "  list_to_remove = list(dictionary)\n",
        "      # Transforms the dictionary to a list \n",
        "\n",
        "  for wrd in list_to_remove:\n",
        "      # looks through each word in the list ' list_to_remove'\n",
        "    if wrd.isalpha() == False:\n",
        "      # Checks for whether element is alphabetical or numerical\n",
        "      del dictionary[wrd]\n",
        "      # if item is numeric, it will drop it from thex list\n",
        "    elif len(wrd) == 1:\n",
        "      del dictionary[wrd]\n",
        "      # removes single character 'words'\n",
        "  dictionary = dictionary.most_common(3000)\n",
        "      # Extracts 3000 most common words\n",
        "  return dictionary\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-VFAwZJ2fBV"
      },
      "source": [
        "# Checking what emails list produces (Not required)\n",
        "onedir = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/train-mails'\n",
        "emails = [os.path.join(onedir,f) for f in os.listdir(onedir)]\n",
        "emails"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFQAQasOGBBP"
      },
      "source": [
        "## Extracting Features and Label Matrix\n",
        "Generates a label and word frequency matrix\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_NG2-TpxQ1j"
      },
      "source": [
        "This function extracts feature columns and populates their values (Feature Matrix of 3000 comumns and rows equal to the number of email files).\n",
        "\n",
        "The function also analyzes the File Names of each email file and decides if it's a Spam or not based on the naming convention. Based on this the function also creates the Labelled Data Column. \n",
        "\n",
        "This function is used to extract the training dataset as well as the testing dataset and returns the Feature Dataset and the Label column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dmVW5xNlyOFc"
      },
      "source": [
        "def extract_features(mail_directory):\n",
        "  email_files = [os.path.join(mail_directory,fi) for fi in os.listdir(mail_directory)]\n",
        "  # Joins path components. Concatenates various path compnents with one directory separator\n",
        "  # os.path.join(path, *paths) ; joins one or more path components/directories\n",
        "  # Joins TEST_directory (mail_directory) with fi (A list of the files inside TEST_directory)\n",
        "  features_matrix = np.zeros((len(email_files),3000))\n",
        "    # Returns a new  2D array with shape of 3000 columns, rows = to the length of the number of emails titled 'features_matrix'\n",
        "  train_labels = np.zeros(len(email_files))    \n",
        "    # 1D array with # of column as the length of files\n",
        "  spam_numb = 0;  \n",
        "  docID = 0;\n",
        "  for one_file in email_files:\n",
        "    with open(one_file) as fi:\n",
        "      # opens each file independently; returns it as a file object\n",
        "      for i, line in enumerate(fi):\n",
        "        # Enumerate returns an index number and the value\n",
        "        if i ==2:\n",
        "          # Looks to the 3rd line of the file because first two lines are irrelevant (Subject and blank).\n",
        "          email_text = line.split()\n",
        "          # Splits each value/word at the spaces to separate each word\n",
        "          for word in email_text:\n",
        "            wordID = 0\n",
        "            # Setting a base word ID to 0 \n",
        "            for i, d in enumerate(dictionary):\n",
        "              if d[0] == word:\n",
        "                # Confirming if the word in the loop is in the dictionary\n",
        "                # If the word is in the dictionary (previously created in the fucntion above), it will be added to the matrix\n",
        "                wordID = i\n",
        "                # 0 is no spam, # 1 is spam, evaluated on naming convention\n",
        "\n",
        "                features_matrix[docID,wordID] = email_text.count(word)\n",
        "      train_labels[docID] = 0;\n",
        "          # Setting a base docID if emial is not spam\n",
        "      filepathTokens = one_file.split('/')\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
        "      if lastToken.startswith(\"spmsg\"):\n",
        "          # If the last work has \"spmsg\", it will be identified as spam and assigned 1\n",
        "        train_labels[docID] = 1;\n",
        "          # if the email is identified as spam, it is reassigned to 1\n",
        "        spam_numb = spam_numb + 1\n",
        "          # spam_numb will count the number of spam messages by adding 1 every time it finds a spam message\n",
        "      docID = docID + 1\n",
        "          # Creating a new docID for non-spam emails\n",
        "  return features_matrix, train_labels\n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76UpSQe2jJWo"
      },
      "source": [
        "## Running the previous functions & printing accuracy scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbOV1Y4hxpiy"
      },
      "source": [
        "The section is the main Program that calls the above two functions and gets executed first. First it \"trains\" the model using model.fit function and Training Dataset. After that it scores the Test Data set by running the Trained Model with the Test Data set. At the end it prints the model performance in terms of accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "134lmhauyQxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e074f69-eac1-48c9-fd49-bb83c1034648"
      },
      "source": [
        "dictionary = my_dictionary(TRAIN_directory)\n",
        "  # Creating 3000 most common words dictionary from the training data\n",
        "\n",
        "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
        "features_matrix, labels = extract_features(TRAIN_directory)\n",
        "test_features_matrix, test_labels = extract_features(TEST_directory)\n",
        "\n",
        "model = GaussianNB()\n",
        "  # Setting model to run the Gaussian Naive Bayes model. This accoutns for normal distribution and supports contonuous data\n",
        "\n",
        "print (\"Training Model using Gaussian Naibe Bayes algorithm .....\")\n",
        "model.fit(features_matrix, labels)  \n",
        "  # Essentially trains the model. Fits it with a features column and labels column\n",
        "print (\"Training completed\")\n",
        "  # Prints Training Completed\n",
        "print (\"testing trained model to predict Test Data labels\")\n",
        "predicted_labels = model.predict(test_features_matrix)\n",
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "print (accuracy_score(test_labels, predicted_labels))\n",
        "  # Runs accuracy score based on predicted labels from the model and actual labels"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9653846153846154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_mPrvN586A"
      },
      "source": [
        "======================= END OF PROGRAM ========================="
      ]
    }
  ]
}